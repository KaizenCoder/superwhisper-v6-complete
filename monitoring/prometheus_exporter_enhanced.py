#!/usr/bin/env python3
"""
Enhanced Prometheus Exporter - Luxa v1.1
==========================================

Exportateur Prometheus complet avec m√©triques VRAM, syst√®me et performance.
"""

import time
import torch
import psutil
import threading
from typing import Dict, Any, Optional
from prometheus_client import (
    Counter, Histogram, Gauge, start_http_server, 
    CollectorRegistry, generate_latest, CONTENT_TYPE_LATEST
)
import http.server
import socketserver

class EnhancedMetricsCollector:
    def __init__(self, port: int = 8000, update_interval: float = 1.0):
        self.port = port
        self.update_interval = update_interval
        
        # Registry personnalis√© pour √©viter les conflits
        self.registry = CollectorRegistry()
        
        # M√©triques pipeline
        self.stt_latency = Histogram(
            'luxa_stt_latency_seconds', 
            'STT processing time in seconds',
            registry=self.registry
        )
        
        self.llm_latency = Histogram(
            'luxa_llm_latency_seconds',
            'LLM processing time in seconds', 
            registry=self.registry
        )
        
        self.tts_latency = Histogram(
            'luxa_tts_latency_seconds',
            'TTS processing time in seconds',
            registry=self.registry
        )
        
        self.llm_tokens_per_second = Gauge(
            'luxa_llm_tokens_per_second', 
            'LLM generation speed in tokens/sec',
            registry=self.registry
        )
        
        self.pipeline_requests = Counter(
            'luxa_pipeline_requests_total', 
            'Total pipeline requests',
            ['status'],  # success, error, timeout
            registry=self.registry
        )
        
        # M√©triques VRAM d√©taill√©es
        self.gpu_memory_used = Gauge(
            'luxa_gpu_memory_used_bytes', 
            'GPU memory used in bytes', 
            ['device', 'device_name'],
            registry=self.registry
        )
        
        self.gpu_memory_free = Gauge(
            'luxa_gpu_memory_free_bytes', 
            'GPU memory free in bytes', 
            ['device', 'device_name'],
            registry=self.registry
        )
        
        self.gpu_memory_total = Gauge(
            'luxa_gpu_memory_total_bytes', 
            'GPU memory total in bytes', 
            ['device', 'device_name'],
            registry=self.registry
        )
        
        self.gpu_utilization = Gauge(
            'luxa_gpu_utilization_percent',
            'GPU utilization percentage',
            ['device', 'device_name'],
            registry=self.registry
        )
        
        self.gpu_temperature = Gauge(
            'luxa_gpu_temperature_celsius',
            'GPU temperature in Celsius',
            ['device', 'device_name'], 
            registry=self.registry
        )
        
        # M√©triques CPU et RAM
        self.cpu_usage = Gauge(
            'luxa_cpu_usage_percent', 
            'CPU usage percentage',
            registry=self.registry
        )
        
        self.cpu_usage_per_core = Gauge(
            'luxa_cpu_usage_per_core_percent',
            'CPU usage per core percentage',
            ['core'],
            registry=self.registry
        )
        
        self.ram_usage = Gauge(
            'luxa_ram_usage_bytes', 
            'RAM usage in bytes',
            registry=self.registry
        )
        
        self.ram_usage_percent = Gauge(
            'luxa_ram_usage_percent',
            'RAM usage percentage', 
            registry=self.registry
        )
        
        self.swap_usage = Gauge(
            'luxa_swap_usage_bytes',
            'Swap usage in bytes',
            registry=self.registry
        )
        
        # M√©triques composants
        self.component_status = Gauge(
            'luxa_component_status',
            'Component status (1=active, 0=inactive)',
            ['component', 'type'],  # component: stt/llm/tts, type: primary/fallback
            registry=self.registry
        )
        
        self.model_load_time = Histogram(
            'luxa_model_load_time_seconds',
            'Model loading time in seconds',
            ['component', 'model_name'],
            registry=self.registry
        )
        
        # M√©triques VAD
        self.vad_latency = Histogram(
            'luxa_vad_latency_seconds',
            'VAD processing time in seconds',
            ['backend'],  # silero, webrtc, none
            registry=self.registry
        )
        
        self.speech_detection_rate = Gauge(
            'luxa_speech_detection_rate',
            'Rate of speech detection (0.0-1.0)',
            registry=self.registry
        )
        
        # Thread pour mise √† jour automatique
        self.update_thread = None
        self.running = False
        
        print(f"üìä Enhanced Metrics Collector initialis√©")
        print(f"üåê Serveur Prometheus: http://localhost:{port}/metrics")
        
    def start_server(self):
        """D√©marre le serveur Prometheus"""
        try:
            # Cr√©er serveur HTTP personnalis√©
            handler = self._create_metrics_handler()
            
            with socketserver.TCPServer(("", self.port), handler) as httpd:
                print(f"‚úÖ Serveur Prometheus d√©marr√© sur port {self.port}")
                
                # D√©marrer thread de mise √† jour
                self.start_background_updates()
                
                # Servir ind√©finiment
                httpd.serve_forever()
                
        except OSError as e:
            if e.errno == 98:  # Address already in use
                print(f"‚ö†Ô∏è Port {self.port} d√©j√† utilis√©, tentative port alternatif...")
                self.port += 1
                self.start_server()
            else:
                print(f"‚ùå Erreur serveur Prometheus: {e}")
                
    def _create_metrics_handler(self):
        """Cr√©e le handler HTTP pour les m√©triques"""
        registry = self.registry
        
        class MetricsHandler(http.server.BaseHTTPRequestHandler):
            def do_GET(self):
                if self.path == "/metrics":
                    # G√©n√©rer m√©triques Prometheus
                    output = generate_latest(registry)
                    
                    self.send_response(200)
                    self.send_header('Content-Type', CONTENT_TYPE_LATEST)
                    self.send_header('Content-Length', str(len(output)))
                    self.end_headers()
                    self.wfile.write(output)
                    
                elif self.path == "/health":
                    # Endpoint de sant√©
                    response = b"OK"
                    self.send_response(200)
                    self.send_header('Content-Type', 'text/plain')
                    self.send_header('Content-Length', str(len(response)))
                    self.end_headers()
                    self.wfile.write(response)
                    
                else:
                    self.send_response(404)
                    self.end_headers()
                    
            def log_message(self, format, *args):
                # Supprimer logs HTTP verbeux
                pass
                
        return MetricsHandler
        
    def start_background_updates(self):
        """D√©marre les mises √† jour automatiques des m√©triques"""
        if self.update_thread and self.update_thread.is_alive():
            return
            
        self.running = True
        self.update_thread = threading.Thread(target=self._update_loop, daemon=True)
        self.update_thread.start()
        print(f"üîÑ Mise √† jour automatique d√©marr√©e (interval: {self.update_interval}s)")
        
    def stop_background_updates(self):
        """Arr√™te les mises √† jour automatiques"""
        self.running = False
        if self.update_thread and self.update_thread.is_alive():
            self.update_thread.join()
        print("üõë Mise √† jour automatique arr√™t√©e")
        
    def _update_loop(self):
        """Boucle de mise √† jour des m√©triques syst√®me"""
        while self.running:
            try:
                self.update_gpu_metrics()
                self.update_system_metrics()
                time.sleep(self.update_interval)
            except Exception as e:
                print(f"‚ö†Ô∏è Erreur mise √† jour m√©triques: {e}")
                time.sleep(self.update_interval)
                
    def update_gpu_metrics(self):
        """Met √† jour toutes les m√©triques GPU"""
        if not torch.cuda.is_available():
            return
            
        try:
            for device_id in range(torch.cuda.device_count()):
                # R√©cup√©rer infos m√©moire
                free, total = torch.cuda.mem_get_info(device_id)
                used = total - free
                
                # Infos device
                props = torch.cuda.get_device_properties(device_id)
                device_label = f"cuda:{device_id}"
                device_name = props.name.replace(" ", "_")
                
                # M√©triques m√©moire
                self.gpu_memory_used.labels(
                    device=device_label, 
                    device_name=device_name
                ).set(used)
                
                self.gpu_memory_free.labels(
                    device=device_label, 
                    device_name=device_name
                ).set(free)
                
                self.gpu_memory_total.labels(
                    device=device_label, 
                    device_name=device_name
                ).set(total)
                
                # Utilisation GPU (n√©cessite nvidia-ml-py pour vraies valeurs) 
                try:
                    import pynvml
                    pynvml.nvmlInit()
                    handle = pynvml.nvmlDeviceGetHandleByIndex(device_id)
                    
                    # Utilisation GPU
                    util = pynvml.nvmlDeviceGetUtilizationRates(handle)
                    self.gpu_utilization.labels(
                        device=device_label,
                        device_name=device_name
                    ).set(util.gpu)
                    
                    # Temp√©rature
                    temp = pynvml.nvmlDeviceGetTemperature(handle, pynvml.NVML_TEMPERATURE_GPU)
                    self.gpu_temperature.labels(
                        device=device_label,
                        device_name=device_name  
                    ).set(temp)
                    
                except ImportError:
                    # Fallback sans pynvml
                    usage_pct = (used / total) * 100
                    self.gpu_utilization.labels(
                        device=device_label,
                        device_name=device_name
                    ).set(usage_pct)
                    
                except Exception as e:
                    print(f"‚ö†Ô∏è Erreur m√©triques GPU {device_id}: {e}")
                    
        except Exception as e:
            print(f"‚ùå Erreur g√©n√©rale m√©triques GPU: {e}")
            
    def update_system_metrics(self):
        """Met √† jour les m√©triques syst√®me"""
        try:
            # CPU global
            cpu_percent = psutil.cpu_percent(interval=0.1)
            self.cpu_usage.set(cpu_percent)
            
            # CPU par core
            cpu_per_core = psutil.cpu_percent(interval=0.1, percpu=True)
            for i, core_usage in enumerate(cpu_per_core):
                self.cpu_usage_per_core.labels(core=f"core_{i}").set(core_usage)
                
            # RAM
            memory = psutil.virtual_memory()
            self.ram_usage.set(memory.used)
            self.ram_usage_percent.set(memory.percent)
            
            # Swap
            swap = psutil.swap_memory()
            self.swap_usage.set(swap.used)
            
        except Exception as e:
            print(f"‚ùå Erreur m√©triques syst√®me: {e}")
            
    # M√©thodes d'enregistrement des m√©triques business
    def record_stt_latency(self, latency_seconds: float):
        """Enregistre la latence STT"""
        self.stt_latency.observe(latency_seconds)
        
    def record_llm_latency(self, latency_seconds: float):
        """Enregistre la latence LLM"""
        self.llm_latency.observe(latency_seconds)
        
    def record_tts_latency(self, latency_seconds: float):
        """Enregistre la latence TTS"""
        self.tts_latency.observe(latency_seconds)
        
    def set_llm_tokens_per_second(self, tokens_per_sec: float):
        """Met √† jour la vitesse de g√©n√©ration LLM"""
        self.llm_tokens_per_second.set(tokens_per_sec)
        
    def increment_pipeline_requests(self, status: str):
        """Incr√©mente le compteur de requ√™tes pipeline"""
        self.pipeline_requests.labels(status=status).inc()
        
    def record_vad_latency(self, latency_seconds: float, backend: str):
        """Enregistre la latence VAD"""
        self.vad_latency.labels(backend=backend).observe(latency_seconds)
        
    def set_speech_detection_rate(self, rate: float):
        """Met √† jour le taux de d√©tection de parole"""
        self.speech_detection_rate.set(rate)
        
    def set_component_status(self, component: str, component_type: str, active: bool):
        """Met √† jour le statut d'un composant"""
        self.component_status.labels(
            component=component, 
            type=component_type
        ).set(1 if active else 0)
        
    def record_model_load_time(self, component: str, model_name: str, load_time_seconds: float):
        """Enregistre le temps de chargement d'un mod√®le"""
        self.model_load_time.labels(
            component=component,
            model_name=model_name
        ).observe(load_time_seconds)
        
    def can_load_model(self, model_size_gb: float, device_id: int = 0) -> bool:
        """V√©rifie si on peut charger un mod√®le de taille donn√©e"""
        if not torch.cuda.is_available():
            return False
            
        try:
            free, _ = torch.cuda.mem_get_info(device_id)
            free_gb = free / 1024**3
            
            # Marge de s√©curit√© de 2GB
            can_load = free_gb > (model_size_gb + 2.0)
            
            print(f"üîç GPU {device_id}: {free_gb:.1f}GB libre, "
                  f"mod√®le {model_size_gb:.1f}GB - {'‚úÖ' if can_load else '‚ùå'}")
                  
            return can_load
            
        except Exception as e:
            print(f"‚ùå Erreur v√©rification mod√®le: {e}")
            return False
            
    def get_current_metrics_summary(self) -> Dict[str, Any]:
        """Retourne un r√©sum√© des m√©triques actuelles"""
        summary = {
            "timestamp": time.time(),
            "system": {},
            "gpu": {}
        }
        
        # M√©triques syst√®me
        try:
            summary["system"] = {
                "cpu_percent": psutil.cpu_percent(),
                "memory_percent": psutil.virtual_memory().percent,
                "memory_used_gb": psutil.virtual_memory().used / 1024**3,
                "memory_total_gb": psutil.virtual_memory().total / 1024**3
            }
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur r√©sum√© syst√®me: {e}")
            
        # M√©triques GPU
        if torch.cuda.is_available():
            try:
                for i in range(torch.cuda.device_count()):
                    free, total = torch.cuda.mem_get_info(i)
                    props = torch.cuda.get_device_properties(i)
                    
                    summary["gpu"][f"gpu_{i}"] = {
                        "name": props.name,
                        "memory_used_gb": (total - free) / 1024**3,
                        "memory_total_gb": total / 1024**3,
                        "memory_free_gb": free / 1024**3,
                        "memory_percent": ((total - free) / total) * 100
                    }
            except Exception as e:
                print(f"‚ö†Ô∏è Erreur r√©sum√© GPU: {e}")
                
        return summary

# Test de l'exportateur Prometheus
def test_prometheus_exporter():
    """Test complet de l'exportateur Prometheus"""
    print("üß™ TEST PROMETHEUS EXPORTER")
    print("="*35)
    
    # Cr√©er collecteur
    collector = EnhancedMetricsCollector(port=8001, update_interval=2.0)
    
    # Test m√©triques manuelles
    print("üìä Test enregistrement m√©triques...")
    collector.record_stt_latency(0.25)
    collector.record_llm_latency(1.5)
    collector.set_llm_tokens_per_second(45.2)
    collector.increment_pipeline_requests("success")
    collector.set_component_status("stt", "primary", True)
    
    # Test mise √† jour syst√®me
    print("üîÑ Test mise √† jour syst√®me...")
    collector.update_gpu_metrics()
    collector.update_system_metrics()
    
    # R√©sum√©
    print("üìã R√©sum√© m√©triques:")
    summary = collector.get_current_metrics_summary()
    for section, data in summary.items():
        if section != "timestamp":
            print(f"  {section}: {data}")
    
    print(f"\n‚úÖ Test termin√©")
    print(f"üåê Pour d√©marrer le serveur: collector.start_server()")

if __name__ == "__main__":
    test_prometheus_exporter() 