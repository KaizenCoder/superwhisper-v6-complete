# STT/stt_handler.py
import torch
import sounddevice as sd
import numpy as np
from transformers import WhisperProcessor, WhisperForConditionalGeneration

class STTHandler:
    def __init__(self, config):
        self.config = config
        self.device = config['gpu_device'] if torch.cuda.is_available() else "cpu"
        
        # Charger le mod√®le Whisper
        model_name = "openai/whisper-base"  # Mod√®le plus l√©ger pour les tests
        self.processor = WhisperProcessor.from_pretrained(model_name)
        self.model = WhisperForConditionalGeneration.from_pretrained(model_name)
        self.model.to(self.device)
        
        self.sample_rate = 16000
        print(f"STT Handler initialis√© avec Whisper sur {self.device}")

    def listen_and_transcribe(self, duration=5):
        """√âcoute le microphone pendant une dur√©e donn√©e et transcrit le son."""
        print("üé§ √âcoute en cours...")
        audio_data = sd.rec(
            int(duration * self.sample_rate),
            samplerate=self.sample_rate,
            channels=1,
            dtype='float32'
        )
        sd.wait()  # Attendre la fin de l'enregistrement
        print("üé§ Enregistrement termin√©, transcription en cours...")
        
        # Pr√©parer l'audio pour Whisper
        audio_input = audio_data.flatten()
        
        # Traitement avec Whisper
        input_features = self.processor(
            audio_input, 
            sampling_rate=self.sample_rate, 
            return_tensors="pt"
        ).input_features.to(self.device)
        
        # G√©n√©ration du texte
        with torch.no_grad():
            predicted_ids = self.model.generate(input_features)
            transcription = self.processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]
        
        print(f"Transcription: '{transcription}'")
        return transcription 